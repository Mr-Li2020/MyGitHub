机器学习复习笔记

第一章

必要的库

- Jupyter Notebook是可以在浏览器中运行代码的交互环境
- NumPy是python科学计算的基础包之一
- Scipy是python中用于科学计算的函数集合
- matplotlib是python主要的科学绘图库
- pandas用于处理和分析数据的python库，它是基于一种叫做DataFrame的数据结构

鸢尾花应用

- load_iris——鸢尾花数据集，load_iris返回一个bunch对象，包含键和值
- DESCR键对应的值是数据集的简要说明
- target_names——字符串数组，里面包含我们要预测的花的品种
- feature_names——字符串列表，对每一个特征进行说明
- data数组每一行对应一朵花，列代表每朵花的四个测量数据

专业术语

- 样本（数据点）：每个实体或者每一行
- 特征：每一列（用来描述这些实体的属性）
- 特征提取（特征工程）：构建良好的数据表征
- 训练数据（训练集）：用于构建机器学习的模型
- 测试数据（测试集、留出集）：用来评估模型性能

模型复杂度和数据集大小的关系

- 数据集越大，构建的模型越精准
- 数据集越小，越容易造成欠拟合  

拟合程度

- 过拟合：构建一个对现有信息来说过于复杂的模型，具有高方差
  例如：将金毛的所有特征来过来构建模型，然后用哈士奇去测试，结果得到哈士奇不是一只狗
- 欠拟合：构建过于简单的模型，具有高偏差
  例如：取猪的两个特征：一个头两个眼睛，然后用狗去验证，结果你说狗就是猪，很显然错了
- 解决过拟合问题   
  - 尽量减少 选取变量的数量
  - 正则化，保留所有特征变量，减少量级或参数θ j的大小
    加入正则项以避免过拟合，即在保持高阶的情况下依旧可以有很好的泛化特性  

第二章 监督学习

K近邻

- 适用于小型数据，易于理解
- 参数：K的大小，数据点之间的距离（欧氏距离）
- 分类：在分类任务中使用”投票法“，即选择这K个样本中出现最多的类别标记作为预测结果
- 回归：在回归任务中使用”平均法“，即将这K个样本的实值输出标记的平均值作为预测结果  
- 优点：模型易于理解
- 缺点：训练集过大，预测速度比较慢，需要对数据进行预处理（稀疏数据集，大多取值为0）

线性模型

- 首选算法，适用于非常大的数据集，也适用于高维数据
- 正则化参数：alpha（回归模型） C（分类模型）
- L1正则化——只有几个重要特征值
- L2正则化——默认
- 回归
  - 线性回归：回归问题最简单的线性方法，均方误差
  - 岭回归——缩减法：处理多特征项，避免过拟合，训练数据过多时，线性回归=岭回归；特征值多时，岭回归>线性回归
  - lasso回归（L1正则化）：参数越小，保留特征值越多；参数越大，保留特征值越少
  特征值少——线性回归
  特征值多——岭回归（L2正则化），若只有几个特征值重要，lasso回归（L1正则化），两者结合效果最好，不易调节
- 正则化：在代价函数后加正则项，加入正则项以避免过拟合，即在高阶的情况下依旧可以有很好的泛化特性

Logistic回归

- 利用正则化来优化逻辑回归，正则化强度的权衡参数C越大，对应的正则化越弱，拟合程度越好；相反，C越小，拟合程度越差。
- Logistic回归和线性支持向量机 :L2正则
- 特点
  - 优点: 计算代价不高，易于理解和实现。
  - 缺点: 容易欠拟合，分类精度可能不高。
  - 适用数据类型: 数值型和标称型数据。
- 多分类算法：一对其余

朴素贝叶斯

- 训练速度快，泛化能力较差
- 朴素贝叶斯模型的泛化能力要比逻辑回归和线性支持向量机稍差
- scikit-learn中实现了三种朴素贝叶斯分类器：GaussianNB、BernoulliNB、MultinomialNB，
  - GaussianNB应用于连续数据，BernoulliNB（假定输入数据为二分类数据）和MultinomialNB（假定输入数据为计数数据）用于文本数据分类。
  - BernoulliNB和MultinomialNB都只有一个参数alpha，alpha用于控制模型复杂度，GaussianNB主要用于高维数据，其他两个主要用于稀疏计数数据，MultinomialNB的性能通常优于BernoulliNB，特别是在包含很多非零特征的数据集上（大型文档）
- 只用于分类，比线性模型速度快，适用于非常大的数据集和高维数据，精度通常低于线性模型

决策树

- 决策树是一种树形结构，其中每个内部结点表示一个属性上的判断，每个分支表示一个测试输出，每个叶结点表示一种类别
- 目的：构造决策树，直到所有叶结点都是纯的叶结点（模型过于复杂，对训练集过拟合）
- 信息熵&信息增益
  - 熵（entropy）： 熵指的是体系的混乱的程度，熵越大，混乱程序越高，纯度越低；熵越小，混乱程度越低，纯度越高
  - 信息增益（information gain）： 在划分数据集前后信息发生的变化称为信息增益。  

- 决策树生成过程
  - 数据如何分割（离散型、连续型）
  - 如何选择分裂属性（贪婪思想）
  - 什么时候停止分裂
- 决策树的构建算法
  - ID3 C4.5——分类树，只能用于离散型数据
  - CART——分类和回归树，既可用于分类，也可用于回归
- 决策树的构造
- 停止分裂
  - 达到最小结点数
  - 熵或基尼值低于阀值
  - 决策树深度达到指定条件
  - 所有特征已使用完毕
- 剪枝：防止过拟合
  - 预剪枝：在树生长的过程中设定一个指标，当达到该指标时就停止生长，容易产生”视觉局限“
  - 后剪枝：树首先充分生长，直到叶结点都有最小的不纯度值为止
- 树的特征重要性：每个特征对树的决策的重要性
- 优点：速度快，不需要数据缩放，可以可视化，很容易理解
- 缺点：即使做了预剪枝，也会出现过拟合，泛化能力差

决策树集成

- 集成：合并多个模型构建更强大模型的方法
- 随机森林：不需要对数据进行预处理（即数据缩放）
  - 解决问题：对训练数据过拟合——方式：对各树取平均值，减少了过拟合，保持了树的预测能力
  - 本质：许多决策树的集合
  - 树的随机化方法：
    - 选择用于构造树的数据点
    - 选择每次划分测试的特征
- 优点：用于回归和分类的随机森林是目前应用最广泛的机器学习方法之一
- 缺点：对于维度非常高的稀疏数据，随机森林的表现往往不是很好

梯度提升回归树

- 与随机森林相比，精度略高，需要内存少，训练速度慢，预测速度快，泛化能力强
- 采用连续的方式构造树（强预剪枝）——限制最大深度（降低过拟合）
- 通常使用深度小（1~5）——占用内存小，预测速度快
- 对参数设置较为敏感

核支持向量机

- 将数据映射到高维
  - 多项式核
  - 径向基函数（高斯核）
- 优点：在各种数据集上表现都很好
- 缺点：
  - 对样本个数的缩放表现不好（对数据缩放敏感）
  - 运行时间和内存使用方面面临挑战
  - 预处理数据和调参需非常小

总结

- K近邻：适用于小型数据集，是很好的基准模型，很容易理解
- 线性模型：非常可靠的首选算法，适用于非常大的数据集，也适用于高维数据
- 朴素贝叶斯：只适用于分类问题，比线性模型速度快，适用于非常大的数据集和高维数据。精度通常低于线性模型
- 决策树：速度很快，不需要数据缩放，可以可视化，很容易理解
- 随即森林：几乎总是比单棵决策树的表现要好，鲁棒性很好，非常强大。不需要数据缩放。不适用于高维稀疏数据
- 梯度提升决策树：精度通常要比随即森林略高。与随机森林相比，训练速度慢，单预测速度快，需要的内存也更少，比随机森林需要更多的参数调节
- 支持向量机：对于特征含义相似的中等大小的数据集很强大，需要数据缩放，对参数敏感

第三章 无监督学习

基本概念

- 无监督学习主要分为数据集变换与聚类
- 无监督变换的一个常见的应用是降维，用较少的特征就可以概括其重要特性，降维的一个常见的应用是为了可视化将数据降为二维
- 聚类算法将数据划分为不同的组，每一组叫做一个簇，每组包含相似的物项
- 无监督学习算法一般用于不包含任何标签信息的数据，所以我们不知道正确的输出应该是什么

预处理与缩放

- 对数据的一种简单的按特征的缩放和移动
- StandardScaler确保每个特征的平均值为0，方差为1，使所有特征都位于同一量级，但这种缩放不能保证特征任何特定的最大值和最小值
- RobustScaler的工作原理和StandardScaler相似，但RobustScaler是使用的是中位数和四分位数，这样RobustScaler会忽略与其他点有很大不同的数据点（比如测量误差）。这些与众不同的数据点也叫做异常值，可能会给其他缩放造成麻烦
- MinMaxScaler移动数据，使所有特征都刚好位于0到1之间。对于二维数据集来说，所有的数据都包含在x轴0到1和y轴0到1组成的矩形中
- Normalizer用到一种完全不同的缩放方法，它对每个数据点进行缩放，使得特征向量的欧式长度等于1
- 使用fit方法拟合缩放器，fit方法计算训练集中每个特征的最大值和最小值，在对缩放器调用fit时只提供了X_train，而不用Y_train
- 对训练数据进行实际缩放，我们使用缩放器的transform方法，在scikit_learn中，每当模型返回数据的一种新表示时，都可以使用transform
- MinMaxScaler（以及其他所有缩放器）总是对训练集和测试集应用完全相同的变换。也就是说，transform方法总是减去训练集的最小值，然后除以训练集的范围

降维、特征提取与流形学习

数据变换的目的：可视化、压缩数据

主成分分析（PCA）

- 主成分分析（PCA）是一种旋转数据集的方法，旋转后的特征在统计上不相关，在做完这种旋转后，根据新特征对解释数据的重要性来选择它的一个子集
- PCA首先找到方差最大的方向，将其标记为“成分1”，这是数据中包含最多信息的方向，沿着这个方向的特征之间最为相关。称为主成分，因为它们是数据方差的主要方向，一般来说，主成分的个数与原始特征相同
- PCA最常见的应用之一就是将高维数据集可视化
- 仅仅保留第一主成分中包含的信息，这种变换有时候用于去除数据中的噪声影响，或者将主成分中保留的那部分信息可视化
- 将PCA对象实例化，调用fit方法找到主成分，然后调用transform来旋转并降维。默认情况下，PCA仅旋转（并移动）数据，但保留所有的主成分。为了降低数据维度，我们需要在创建PCA对象时指定想要保留的主成分个数
- PCA的一个缺点在于，通常不容易对图中两个轴做出解释。主成分对应于原始数据中的方向，所以他们是原始特征的组合。在拟合过程中，主成分被保存在PCA对象的components_属性中
- components_中每一行对应一个主成分，他们按照重要性排序（第一主成分排在首位，以此类推）。列对应于PCA的原始特征属性
- PCA的另一个应用是特征提取，它的思想是：可以找到一种数据表示，比给定的原始表示更适合于分析。特征提取很有用，它的一个很好的应用就是图像。
- PCA的白化选项，他将主成分缩放到相同的尺度。变换后的结果与使用StandardScaler相同，白化不仅对应于旋转数据，还对应于缩放数据使其形状是圆形而不是椭圆

非负矩阵分解（NMF）

- 其目的在于提取有用的特征，工作原理类似于PCA，也可以用于降维。这种方法只能应用于每个特征都是非负的数据
- 与PCA相比，NMF得到的分量更容易解释，因为负的分量和系数可能会导致难以解释的抵消效应
- NMF使用了随机初始化，根据随机种子的不同可能会产生不同的结果
- NMF的主要参数是我们想要提取的分量个数

用t-SNE进行流形学习

- 允许进行更复杂的映射，通常也可以给出更好的可视化，用于可视化
- t-SNE算法的思想：找到数据的一个二维表示，尽可能地保持数据点之间的距离，让在原始特征空间中距离较近的点更加靠近，距离较远的点更加远离，t-SNE重点关注距离较近的点，而不是保持距离较远的点之间的距离。换句话说，它试图保存那些表示哪些点比较靠近的信息
- t-SNE不支持变换新数据，所以TSNE类中没有transform方法。我们可以调用fit_transform方法来代替，它会构建模型并立刻返回变换后的数据
- t-SNE算法有一些调节参数，你可以尝试修改，但一般作用很小

聚类

- 聚类是将数据集划分成组的任务，这些组叫簇，其目标是划分数据，使得一个簇内的数据点非常相似且不同簇内的数据点非常不同，可用于数据分割，也可用于离群点检测

K均值聚类

- 它试图找到代表数据特定区域的簇中心，算法交替执行以下两个步骤：将数据点分配给最近的簇中心，然后将每个簇中心设置为所分配的所有数据点的平均值。如果簇的分配不再发生变化，那么算法结束
- 需要初始化K的大小，聚类的初始化不同，得到的结果不同
- kmeans = KMeans(n_clusters = 3)，如果不指定n_clusters，它的默认值是8
- 优点：不仅相对容易理解和实现，而且运行速度也相对较快。K均值可以轻松扩展到大型数据集，可以处理非常大的数据集
- 缺点：依赖于随机初始化，容易造成局部最优，另一个缺陷是对簇形状的假设的约束性很强，而且还要求指定所要寻找的簇的个数

凝聚聚类

- 凝聚聚类指的是许多基于相同原则构建的聚类算法，这一原则是：算法首先声明每个点是自己的簇，然后合并两个最相似的簇，直到满足某种停止准则为止。scikit_learn中实现的停止准则是簇的个数，因此相似的簇被合并，直到仅剩下指定个数的簇。还有一些链接准则：
  - ward:默认选项，ward挑选两个簇来合并，使得所有簇中的方差增加最小。这通常会得到大小差不多相等的簇。适用于大多数数据集
  - average：average链接将簇中所有点之间平均距离最小的两个簇合并
  - complete：complete链接（最大距离链接）将簇中之间距离最小的两个簇合并
- 凝聚聚类生成了所谓的层次聚类，将层次聚类可视化——树状图（dendrogram），SciPy提供一个函数，接受数据数组x并计算出一个链接数组，它对层次聚类的相似度进行编码，然后我们可以将这个链接数组提供给SciPy的dendrogram函数来绘制树状图

DBSCAN

- DBSCAN——具有噪声的基于密度的空间聚类应用
- 优点：不需要用户先验地设置簇的个数，可以划分具有复杂形状的簇，还可以找出不属于任何簇的点，DESCAN比凝聚聚类和k均值稍慢，但仍然可以扩展到相对较大的数据集。
- DBSCAN有两个参数：min_samples和eps，如果在距一个给定数据点eps的距离内至少有min_samples个数据点，那么这个数据点就是核心样本。DBSCAN将彼此距离小于eps的核心样本放到同一个簇中
- 原理：算法首先任意选取一个点，然后找到到这个点的距离小于eps的所有的点。如果距起始点的距离在eps之内的数据点个数小于min_samples，那么 这个点被标记为噪声，也就是说他不属于任何簇。如果距离在eps之内的数据点个数大于min_samples，则这个点被标记为核心样本，并被分配一个新的簇标签。然后访问该点的所有邻居（距离在eps之内）。如果他们还没有被分配一个簇，那么就将刚刚创建的新的簇标签分配给他们。如果他们是核心样本，那么就依次访问其邻居，依次类推。簇逐渐增大，直到在簇的eps距离内没有更多的核心样本为止。然后选取另一个未被访问过的点，并重复相同的过程
- 一共有三种类型的点：核心点、与核心点距离在eps之内的点（边界点）和噪声。如果DBSCAN算法在特定数据集上多次运行，那么核心点的聚类始终相同，同样的点也始终被标记为噪声。但边界点可能与不止一个簇的核心样本相邻，因此，边界点所属的簇依赖于数据点的访问顺序。

聚类算法的评估

- 轮廓系数：轮廓系数计算一个簇的紧致度，其值越大越好，K均值的轮廓系数最高
- rand指数和归一化互信息

总结

- K均值和凝聚聚类允许你指定想要的簇的数量，而DBSCAN允许你用eps参数定义接近程度，从而间接影响簇的大小。三种方法都可以用于大型的现实世界数据集，都相对容易理解，也都可以聚类成多个簇

第四章 数据表示与特征工程

分类变量

One-Hot编码（虚拟变量）（N取一编码）

- 思想：将一个分类变量替换为一个或多个新特征，新特征取值为0和1
- 将数据转换为分类变量的one-hot编码有两种方法：一种是使用pandas，一种是使用scikit_learn
- pandas编码数据有一种非常简单的方法，就是使用get_dummies函数，get_dummies函数自动变换所有具有对象类型的列或所有分类的列
- 使用values属性将data_dummies数据框转换为NumPy数组
- pandas的get_dummies函数将所有数字看作是连续的，不会为其创建虚拟变量。为了解决这个问题，你可以使用scikit_learn的OneHotencoder，指定哪些变量是连续的、哪些变量是离散的，你也可以将数据框中的数值列转换为字符串

分箱、离散化、线性模型与树

- 特征分箱：可以让线性模型在连续数据上变得更加强大，将其划分为多个特征
- 分箱特征对基于树的模型通常不会产生更好的效果

交互特征与多项式特征

- 想要丰富特征表示，另一种方法是添加原始数据的交互特征和多项式特征
- 使用Ridge时，交互特征和多项式特征对性能有很大的提升。但使用更加复杂的模型（比如随机森林），情况会稍有不同
- 调用get_feature_names方法来获取特征的语义
- 将多项式特征和线性回归模型一起使用，可以得到经典的多项式回归

单变量非线性变换

- log和exp函数可以帮助调节数据的相对比例
- 在处理具有周期性模式的数据时，sin和cos函数非常有用
- 分箱、多项式和交互式都对模型在给定数据集上的性能有很大影响

自动化特征选择

单变量统计

- 它们只单独考虑每个特征，因此，如果一个特征只有在与另一个特征合并时才具有信息量，那么这个特征将被舍弃
- 在scikit_learn中使用单变量特征选择，你需要选择一项测试——对分类问题通常是f_classif（默认值），对回归问题通常是f_regression——然后基于测试中确定的p值来选择一种舍弃特征的办法。所有舍弃参数的方法都是使用阈值来舍弃所有p值过大的特征值，计算阈值的方法不同，最简单的是SelectKBest和SelectPercentile，前者固定数量的k个特征，后者选择固定百分比的特征

基于模型的特征选择

- 选择使用一个监督机器学习模型来判断每个特征的重要性，并且仅保留最重要的特征。用于特征选择的监督模型不需要与用于最终监督建模的模型相同。特征模型需要为每个特征提供某种重要性度量，以便用这个度量对特征进行排序
- 决策树和基于决策树的模型提供了feature_importances_属性，可以直接编码每个特征的重要性。线性模型系数的绝对值也可以用于表示特征重要性
- 要想使用基于模型的特征选择，我们需要使用SelectFromModel变换器
- SelectFromModel类选出重要度量（由监督模型提供）大于给定阈值的所有特征

迭代特征选择

- 构建一系列模型，每个模型都使用不同数量的特征
- 有两种基本方法
  - 开始时没有特征，然后逐个添加特征，直到满足某个终止条件；或者从所有特征开始，然后逐个删除特征，直到满足某个终止条件
  - 递归特征消除：它从所有特征开始构建模型，并根据模型舍弃最不重要的特征，然后使用除被舍弃特征之外的所有特征来构建一个新模型，如此继续，直到仅剩下预设数量的特征

第五章 模型评估与改进

交叉验证

- 交叉验证是一种评估泛化性能的统计学方法，它比单次划分训练集和测试集的方法更稳定、全面
- 最常用的交叉验证是K折交叉验证，其中k是由用户指定的数字，通常取5或8，在执行5折交叉验证时，首先将数据划分为（大致）相等的5部分，每一部分叫做折
- scikit_learn是利用model_selection模块中的cross_val_score函数来实现交叉验证的，cross_val_score函数的参数是我们想要评估的模型，训练数据和真实标签
- 优点：对数据的使用更加高效；提供我们的模型对训练集选择的敏感性信息
- 缺点：增加了计算成本
- 交叉验证不是一种构建可应用于新数据的模型方法。交叉验证不会返回一个模型。在调用cross_val_score函数时，内部会构建多个模型，但交叉验证的目的只是评估给定算法在特殊数据集上训练后的泛化性能好坏

分层k折交叉验证

- 在分层交叉验证中，我们划分数据，使每折中类别之间的比例与整个数据集中的比例相同
- 利用cv参数来调节cross_val_score所使用的折数，但scikit_learn允许提供一个交叉验证分离器作为cv参数，来对数据划分进行更精细的控制

留一法交叉验证（leave-one-out）

- 每折只包含单个样本的k折交叉验证，每次划分选择单个数据点作为数据集
- 对大型数据集来说非常耗时，对于小型数据集有时可以给出更好的评估结果

打乱划分交叉验证（ShuffleSplit）

- 每次划分为训练集取样train_size个点，为测试集取样test_size个点。将这一划分方法重复n_iter次
- ShuffleSplit还有一种分层形式，其名称是StratifiedShuffleSplit，他可以为分类任务提供更可靠的结果
- 可以在训练集和测试集大小之外独立控制迭代次数，还允许在每次迭代中仅使用部分数据，这可以通过设置train_size和test_size之和不等于1来实现

分组交叉验证

- 适用于数据中分组高度相关，例如：将一个人的所有情感划分为一个分组

网格搜索

- 它主要是指尝试我们关心的参数的所有可能组合

简单网格搜索

参数过拟合的风险与验证集

- 利用训练集和验证集的组合完成所有的探索性分析和模型选择，并保留测试集用于最终评估

交叉验证的网格搜索

- 交叉验证的网格搜索是一种常用的调参方法，因此scikit_learn提供了GridSearchCV类，它以估计器（estimator）的形式实现类这种方法
- 字典的值是我们想要尝试的参数设置
- 字典的键是我们想要调节的参数名称
- 交叉验证最佳精度保存在best_score_中
- 可以用best_estimator_属性来访问最佳参数对应的模型
- SVC有一个kernel参数，如果kernel='linear'，那么模型是线性的，只会用到C参数，如果kernel='rbf'，则需要使用C和gamma两个参数
- 在scikit_learn中实现嵌套交叉验证——调用cross_val_score
- scikit_learn不允许并行操作的嵌套

评估指标与评分

- 商业指标：应用的高级目标
- 商业影响：选择特定算法的结果
- 第一类错误：假正例
- 第二类错误：假反例
- 不平衡数据集（具有不平衡类别的数据集）：一个类别比另一个类别出现次数多很多的数据集
- 真正例：正类中正确分类的样本
- 真反例：反类中正确分类的样本
- 精度：正确预测的数量除以所有样本的数量
- 
